---
title: Presentation for 2023 honours correspondence analysis section
format:
  revealjs:
    embed-resources: true
---

## Introduction to correspondence analysis (CA)

- Context: $\mathbf{Y}$ (counts)
  - Goal: Graphically display the relationships between the rows and columns

:::: {.columns}

::: {.column width="50%"}

```{r}
#| results: asis
#| echo: false
data("smoke", package = "ca")
smoke |> knitr::kable()
```

:::

::: {.column width="50%"}

```{r}
#| echo: false
plot(
  ca(smoke), mass = TRUE, contrib = "absolute",
  map = "rowprincipal", arrows = rep(TRUE, 2)
  )
```

:::

::::

## Example applications {.smaller}

**Datasets**
asdfasdf
- Rows are various dams, and columns are counts of waterbird species
- Rows are various immune compartments (e.g. blood, spleen, lymph), and columns are frequencies of immune cell types (e.g. T cells, B cells, NK cells)
- Rows are company brands (e.g. Cadbury, Beacon, Lindt), and columns are consumer ratings on a 1-5 scale (e.g. quality, price, taste)

**Key characteristics**

- Non-negative
- Natural zero (i.e. zero means literally nothing and not simply that two quantities are equal, for example)
- Same units (e.g. counts all in thousands)

## Correspondence matrix, $\mathbf{P}$ {.smaller}

- Suppose that we have some matrix $X:I \times J$ where each element 
  - Rows can be thought of as observations and columns as variables

- The correspondence matrix $\mathbf{P}:I \times J$ is the matrix of overall proportions where
$$
P_{ij}= \frac{x_{ij}}{\sum_{i=1}^I \sum_{j=1}^J x_{ij}} = \frac{x_{ij}}{n}
$$


## Correspondence analysis as matrix approximation

- Let $\mathbf{r}$ be the vector of row totals, i.e. $r_i=\sum_{j=1}^n P_{ij} = \mathbf{P} \mathbf{1}$
- Let $\mathbf{c}$ be the vector of column totals, i.e. $c_j=\sum_{i=1}^m P_{ij} = \mathbf{P}' \mathbf{1}$
- Let $\mathbf{D}_r$ be the diagonal matrix of row totals, i.e. $D_{r_{ii}}=r_i$ and $\mathbf{D}_c$ the diagonal matrix of column totals

::: {.callout-important}
- Correspondence analysis can be viewed as a weighted least squared problem to select a low-rank $\hat{P}$ such that the following is minimised:

\begin{equation*}
$\sum_{i=1}^I\sum_{j=1}^J \frac{(p_{ij} - \hat{p}_{ij})^2}{r_ic_j}$, \text{or equivalently} \\
\mathrm{tr}[(\mathbf{D}_r^{-1/2}(\mathbf{P} - \hat{\mathbf{P}})\mathbf{D}_c^{-1/2})(\mathbf{D}_r^{-1/2}(\mathbf{P} - \hat{\mathbf{P}})\mathbf{D}_c^{-1/2})']
\end{equation*}

:::

## $\chi^2$ distance

- The $\chi^2$ distance is a measure for comparing two entities
- For example:
  - Comparing two histograms with equal bin placements
  - Comparing two densities
- In our case, we're comparing the observed counts (which are like a two-way histogram) and expected counts
- Formula:
  - $\sum_{ij}\frac{(x_{ij} - e_{ij})^2}{e_{ij}}$


## Physical interpretation

- The row and column weights ($\mathbf{r}$ and $\mathbf{c}$) are termed *masses*


## Inertia

Source: https://en.wikipedia.org/wiki/Correspondence_analysis

- Inertia isa  weighted covariance
  - Sum of squared singular values is the total inertaa of the data table ($S$)
  - What is this equal to?
    - Sum of all elements in $S$, squared
  - 

## CA vs PCA 

- Association between rows and columns
  - PCA has that?
- Interested in profiles
  - PCA might standardise the variables, but not the rows as well
- 

## Examples of contexts for correspondence analysis {.smaller}
